# 4장: Edge Impulse를 이용한 음성 제어 LED

이 장에서는 Edge Impulse를 사용하여 음성 명령으로 LED를 제어하는 방법을 설명합니다. "on", "off"와 같은 간단한 음성 명령을 인식하여 LED를 켜고 끄는 TinyML 애플리케이션을 구축하는 과정을 다룹니다.

## 프로젝트 개요

*   **목표**: 음성 명령("on", "off")을 인식하여 LED를 제어합니다.
*   **사용 도구**:
    *   마이크로컨트롤러 보드 (예: Arduino Nano 33 BLE Sense)
    *   마이크 (보드에 내장 또는 외장)
    *   Edge Impulse Studio (데이터 수집, 모델 훈련 및 배포용)
    *   Arduino IDE (마이크로컨트롤러 프로그래밍용)

## 개발 과정

### 1. 데이터 수집

*   Edge Impulse CLI를 사용하여 마이크로컨트롤러에서 직접 음성 데이터를 수집합니다.
*   "on", "off" 각 레이블에 대해 여러 샘플을 녹음합니다.
*   다양한 배경 소음과 발음을 포함하여 "unknown" 카테고리에 대한 데이터도 수집하여 모델의 강건성을 높입니다.

### 2. 임펄스(Impulse) 설계

Edge Impulse Studio에서 머신러닝 파이프라인(임펄스)을 설계합니다.

*   **입력 블록**: 원시 오디오 데이터 (시간 시계열 데이터).
*   **처리 블록 (Preprocessing)**: 오디오 특징을 추출하기 위해 MFCC(Mel-frequency cepstral coefficients)를 사용합니다. MFCC는 인간의 청각 시스템을 모방하여 오디오 데이터를 효과적으로 표현하는 방법입니다.
*   **학습 블록 (Learning)**: 추출된 특징을 분류하기 위해 작은 합성곱 신경망(CNN)을 사용합니다. CNN은 오디오 특징의 시간적 패턴을 학습하는 데 적합합니다.
*   **출력**: "on", "off", "unknown" 세 가지 클래스(레이블)에 대한 확률.

### 3. 모델 훈련

*   설계된 임펄스를 사용하여 모델을 훈련합니다.
*   Edge Impulse는 훈련 과정과 모델 성능(정확도, 손실)을 시각적으로 보여줍니다.
*   혼동 행렬(confusion matrix)을 통해 모델이 어떤 클래스를 잘 구분하고 어떤 클래스를 혼동하는지 분석할 수 있습니다.

### 4. 모델 테스트

*   훈련에 사용되지 않은 테스트 데이터를 사용하여 모델의 성능을 검증합니다.
*   Edge Impulse의 "Live classification" 기능을 사용하여 실시간으로 마이크 입력을 통해 모델을 테스트할 수 있습니다.

### 5. 모델 배포

*   훈련된 모델을 마이크로컨트롤러에 배포할 수 있도록 최적화된 C++ 라이브러리로 내보냅니다.
*   이 라이브러리에는 추론을 실행하는 데 필요한 모든 코드(신호 처리, 신경망)가 포함되어 있습니다.

### 6. 아두이노 애플리케이션 작성

*   Arduino IDE에서 새 스케치를 작성합니다.
*   Edge Impulse에서 내보낸 C++ 라이브러리를 가져옵니다.
*   다음과 같은 로직을 구현합니다.
    1.  마이크에서 오디오 데이터를 지속적으로 샘플링합니다.
    2.  수집된 데이터에 대해 추론을 실행하여 음성 명령을 인식합니다.
    3.  인식 결과("on" 또는 "off")에 따라 LED의 상태를 변경합니다.
*   코드를 마이크로컨트롤러에 업로드하고 실행하여 음성으로 LED를 제어합니다.

## 결론

이 장의 과정을 통해 데이터 수집부터 모델 훈련, 배포, 그리고 최종 애플리케이션 작성까지 음성 인식을 위한 전체 TinyML 워크플로우를 경험할 수 있습니다. 이를 통해 다른 종류의 소리나 단어를 인식하는 프로젝트로 확장할 수 있는 기반을 다질 수 있습니다.
